{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tariqsoft/content/blob/main/Anomaly_based_IDS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hide_input": true,
        "id": "CZLXr_wZu0od"
      },
      "source": [
        "<span style=\"font-size:3em; text-align:center\">Information System Security -</span>\n",
        "\n",
        "<span style=\"font-size:3em; text-align:center\">Anomaly-based Intrusion Detection System</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9Ig4Dztu0oi"
      },
      "source": [
        "Data is contained in 8 different CSV files, each containing different attack data at different times. So first thing we must do is merge all the data from files into one pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RH3zp5Dyu0ok"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dyE_-SQUu0ol"
      },
      "outputs": [],
      "source": [
        "# Saving all .csv files in folder to list.\n",
        "path = \"MachineLearningCVE/\"\n",
        "files = [file for file in glob.glob(path + \"**/*.csv\", recursive=True)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "hide_input": false,
        "scrolled": false,
        "id": "J4Ji6hPYu0om",
        "outputId": "48c2f181-64ce-448e-ec2f-2f3659e20716",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "[print(f) for f in files]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "scrolled": false,
        "id": "tZ3SSaL8u0on"
      },
      "outputs": [],
      "source": [
        "# Reading all the csv files into dataframes and putting thoose DFs to one list.\n",
        "\n",
        "dataset = [pd.read_csv(f) for f in files]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "scrolled": true,
        "id": "wmCH_fSvu0oo"
      },
      "outputs": [],
      "source": [
        "# Here we can see the number of rows and columns for each table.\n",
        "\n",
        "for d in dataset:\n",
        "    print(d.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "H6p7VNPCu0oo",
        "outputId": "acbbe67b-f0dc-47ec-cc48-578764248a33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'same_columns' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-574034fa9d02>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msame_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'same_columns' is not defined"
          ]
        }
      ],
      "source": [
        "# We already established that all tables have the same number of columns, but are they the same columns?\n",
        "# This next piece of code loops over all given tables and compares each of them to all others.\n",
        "\n",
        "for i in range(0,len(dataset)):\n",
        "    if i != len(dataset)-1:\n",
        "        same_columns = dataset[i].columns == dataset[i+1].columns\n",
        "\n",
        "        if False in same_columns:\n",
        "            print(i)\n",
        "            break\n",
        "\n",
        "same_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkwudmdSu0op"
      },
      "outputs": [],
      "source": [
        "# Combining all tables into one dataset. This is possilbe since all tables have the same columns,\n",
        "# as we checked in the cell above.\n",
        "\n",
        "dataset = pd.concat([d for d in dataset]).drop_duplicates(keep=False)\n",
        "dataset.reset_index(drop=True, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ySnpId7Wu0oq"
      },
      "outputs": [],
      "source": [
        "# By checking the shape of dataset we can confirm that concatenation has been successfull.\n",
        "\n",
        "dataset.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "FU7twJ8Cu0oq"
      },
      "source": [
        "# Preliminary data analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "FWJJMMTeu0oq"
      },
      "source": [
        "Some general info about the dataset. It contains roughly 2.5 million records across 79 columns. Data consists of mostly int64 and float64 types, except 3 attributes of 'object' type.\n",
        "\n",
        "Dataset contains of network traffic data during different attacks, represented with values like: port numbers, IP adressses, packet lenghts, SYN/ACK/FIN/.. flag counts, packet size and other..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "hidden": true,
        "id": "OoNW-J4Nu0or"
      },
      "outputs": [],
      "source": [
        "#dataset = pd.read_csv('Dataset_clean.csv', index_col=[0])\n",
        "dataset.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "hidden": true,
        "id": "RR8L88LKu0or"
      },
      "outputs": [],
      "source": [
        "dataset.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "SkCTajj7u0os"
      },
      "source": [
        "Upon further inspection we can see that dataset contains 15 labels. Labels represent network/web attacks and BENIGN state which is the network traffic during normal business day."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "scrolled": false,
        "id": "JAv9LX3tu0os"
      },
      "outputs": [],
      "source": [
        "# Dataset conatains 15 labels.\n",
        "#print(dataset[' Label'].unique())\n",
        "#len(dataset[' Label'].unique())\n",
        "\n",
        "print(dataset['Label'].unique())\n",
        "len(dataset['Label'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "1enBFKT9u0os"
      },
      "outputs": [],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "k-iiSiY4u0os"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "DLuGUzeHu0os"
      },
      "source": [
        "Most records in the dataset are of DDos and DOS Hulk attacks. This might pose a problem later in model training, considering that there is a very small amount of data for most attacks. Model selection will be greatly influenced by this information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "Gfjm_HPzu0ot"
      },
      "outputs": [],
      "source": [
        "data = dataset['Label'].where(dataset['Label'] != \"BENIGN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "hide_input": false,
        "id": "ULcEMGauu0ot"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "chart = sns.countplot(data, palette=\"Set1\")\n",
        "plt.xticks(rotation=45, horizontalalignment=\"right\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "zq-pQoR7u0ot"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "hVCEIduKu0ot"
      },
      "source": [
        "This chapter contains data cleaning code. We go through the process of renaming columns, removing NaN and non-finite values (-inf, inf) to get the data ready for visualization and model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "zyELmELwu0ot"
      },
      "source": [
        "## Renaming columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "zscPUtruu0ou"
      },
      "outputs": [],
      "source": [
        "# Removing whitespaces in column names.\n",
        "\n",
        "col_names = [col.replace(' ', '') for col in dataset.columns]\n",
        "dataset.columns = col_names\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "JRlzdTMJu0ou"
      },
      "outputs": [],
      "source": [
        "# Here we can see that 'Label' column contains some wierd characters.\n",
        "\n",
        "dataset[\"Label\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "wRrCWXbCu0ou"
      },
      "outputs": [],
      "source": [
        "# This next snippet uses regular expressions to replace wierd characters with dunders.\n",
        "\n",
        "label_names = dataset['Label'].unique()\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "label_names = [re.sub(\"[^a-zA-Z ]+\", \"\", l) for l in label_names]\n",
        "label_names = [re.sub(\"[\\s\\s]\", '_', l) for l in label_names]\n",
        "label_names = [lab.replace(\"__\", \"_\") for lab in label_names]\n",
        "\n",
        "label_names, len(label_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "-0dDWr1ju0ou"
      },
      "outputs": [],
      "source": [
        "# Replacing 'Label' column values with new readable values.\n",
        "\n",
        "labels = dataset['Label'].unique()\n",
        "\n",
        "for i in range(0,len(label_names)):\n",
        "    dataset['Label'] = dataset['Label'].replace({labels[i] : label_names[i]})\n",
        "\n",
        "dataset['Label'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "scrolled": true,
        "id": "d5Nynivuu0ov"
      },
      "outputs": [],
      "source": [
        "len(dataset['Label'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "1XVVUSLEu0ov"
      },
      "outputs": [],
      "source": [
        "# Saving cleaned dataset.\n",
        "\n",
        "#dataset.to_csv(\"Dataset_clean.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "A92CFsy5u0ov"
      },
      "source": [
        "## Removing NULL values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "s01nIoWzu0ov"
      },
      "outputs": [],
      "source": [
        "#dataset = pd.read_csv(\"Dataset_clean.csv\", index_col=0)\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "vbVf0Nn0u0ov"
      },
      "outputs": [],
      "source": [
        "# Checking if there are any NULL values in the dataset.\n",
        "\n",
        "dataset.isnull().values.any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "fbV2KKv3u0ow"
      },
      "outputs": [],
      "source": [
        "# Checking which column/s contain NULL values.\n",
        "\n",
        "[col for col in dataset if dataset[col].isnull().values.any()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "XiclEQTwu0ow"
      },
      "outputs": [],
      "source": [
        "# Checking how many NULL values it this column contains.\n",
        "\n",
        "dataset['FlowBytes/s'].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "6mLzQLc7u0ow"
      },
      "outputs": [],
      "source": [
        "# Considering that only 334 rows contain NULL vlaues in the entire dataset, which makes about 0.01%, we\n",
        "# can safely remove all NULL rows without spoiling the data.\n",
        "\n",
        "334/dataset.shape[0]*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "scrolled": true,
        "id": "fNbO9Dreu0ow"
      },
      "outputs": [],
      "source": [
        "# Removing rows that contain NULL values and checking if number of removed rows is equal to the number of null values.\n",
        "\n",
        "before = dataset.shape\n",
        "\n",
        "dataset.dropna(inplace=True)\n",
        "\n",
        "after = dataset.shape\n",
        "\n",
        "before[0] - after[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "scrolled": true,
        "id": "3ZHvIEgfu0ox"
      },
      "outputs": [],
      "source": [
        "dataset.isnull().any().any()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "Pun8hETWu0ox"
      },
      "source": [
        "## Removing *non-finite* values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "t9doZ2gIu0o2"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "XAlgVaXJu0o3"
      },
      "outputs": [],
      "source": [
        "labl = dataset['Label']\n",
        "dataset = dataset.loc[:, dataset.columns != 'Label'].astype('float64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "hv4lChnwu0o3"
      },
      "outputs": [],
      "source": [
        "# Checking if all values are finite.\n",
        "\n",
        "np.all(np.isfinite(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "Gcd2wZeNu0o3"
      },
      "outputs": [],
      "source": [
        "# Checking what column/s contain non-finite values.\n",
        "\n",
        "nonfinite = [col for col in dataset if not np.all(np.isfinite(dataset[col]))]\n",
        "\n",
        "nonfinite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "hidden": true,
        "id": "aucf6eYJu0o3",
        "outputId": "fccd61d3-fa57-4cdd-9479-b6dabb2947e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-f9f542976c9c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Checking how many non-finite values each column contains.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfinite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FlowBytes/s'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfinite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "# Checking how many non-finite values each column contains.\n",
        "\n",
        "finite = np.isfinite(dataset['FlowBytes/s']).sum()\n",
        "\n",
        "dataset.shape[0] - finite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "6bBYB4aNu0o4"
      },
      "outputs": [],
      "source": [
        "# Checking how many non-finite values each column contains.\n",
        "\n",
        "finite = np.isfinite(dataset['FlowPackets/s']).sum()\n",
        "\n",
        "dataset.shape[0] - finite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "FM63n6nVu0o4"
      },
      "outputs": [],
      "source": [
        "# Same as before, since there is a small number of non-finite values we can safely remove them from the dataset\n",
        "# without spoiling the dataset.\n",
        "\n",
        "# Replacing infinite values with NaN values.\n",
        "dataset = dataset.replace([np.inf, -np.inf], np.nan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "K4F611tCu0o4"
      },
      "outputs": [],
      "source": [
        "# We can see that now we have Nan values again.\n",
        "\n",
        "np.any(np.isnan(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "1uEAJk7qu0o4"
      },
      "outputs": [],
      "source": [
        "# Bringing the Labels back into the dataset before deliting Nan rows.\n",
        "\n",
        "dataset = dataset.merge(labl, how='outer', left_index=True, right_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "ryNvzNN5u0o4"
      },
      "outputs": [],
      "source": [
        "# Removing new NaN values.\n",
        "\n",
        "dataset.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "scrolled": true,
        "id": "PH3LIKthu0o4"
      },
      "outputs": [],
      "source": [
        "dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "-8yTlTAsu0o5"
      },
      "outputs": [],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "YB0SKHQdu0o5"
      },
      "outputs": [],
      "source": [
        "# Saving cleaned dataset.\n",
        "\n",
        "#dataset.to_csv(\"Dataset_clean_dropna.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "rM9ygN6Cu0o5"
      },
      "source": [
        "# Data visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "swAD2BjQu0o5"
      },
      "source": [
        "So, by now we know our dataset has 78 features and is split into 15 categories (14 attacks and 1 \"normal\" state).\n",
        "Next step is to try and visualize what the dataset looks like in feature space.\n",
        "For this we will use principal component analysis (PCA) to reduce dimensionality and then pass the reduced dataset to t-SNE (t - Distributed Stohastic Neighbor Entities) for visual representation in 2D space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "28BxkFjWu0o6"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "bhdNyRRzu0o6"
      },
      "outputs": [],
      "source": [
        "# We are going to pick 10.000 random rows from the dataset for visualization purposes.\n",
        "# Setting the random seed for reproducability of results.\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "rand_perm = np.random.permutation(dataset.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "hidden": true,
        "id": "fiLDtVNFu0o6"
      },
      "outputs": [],
      "source": [
        "feature_cols = dataset.columns[:-1]\n",
        "\n",
        "dataset_subset = dataset.loc[rand_perm[:10000],:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "KRZ8iZpyu0o6"
      },
      "outputs": [],
      "source": [
        "dataset_subset = dataset_subset.replace([np.inf, -np.inf], np.nan)\n",
        "dataset_subset.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "kw519c27u0o7"
      },
      "outputs": [],
      "source": [
        "data_subset = dataset_subset[feature_cols].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "ec_UzNNZu0o7"
      },
      "outputs": [],
      "source": [
        "# Performing the principal component analysis. With just 19 components the variance ratio remains 99%, which is great.\n",
        "\n",
        "pca = PCA(n_components=19)\n",
        "pca_res = pca.fit_transform(data_subset)\n",
        "\n",
        "data_subset = None\n",
        "np.sum(pca.explained_variance_ratio_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "mpFT-RCHu0o7"
      },
      "outputs": [],
      "source": [
        "# Computing t-SNE.\n",
        "\n",
        "tsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=1000)\n",
        "tsne_res = tsne.fit_transform(data_subset)\n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "K6CIWra-u0o7"
      },
      "outputs": [],
      "source": [
        "dataset_subset['tsne_firstD'] = tsne_res[:,0]\n",
        "dataset_subset['tsne_secondD'] = tsne_res[:,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "hidden": true,
        "hide_input": false,
        "scrolled": false,
        "id": "xQEfawsRu0o8",
        "outputId": "0888ef98-4558-4960-e106-fa400bc26ada",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-9f5c31fd3789>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m sns.scatterplot(\n\u001b[1;32m      4\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tsne_firstD\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tsne_secondD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpalette\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor_palette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ],
      "source": [
        "plt.figure(figsize=(16,16))\n",
        "\n",
        "sns.scatterplot(\n",
        "    x=\"tsne_firstD\", y=\"tsne_secondD\",\n",
        "    palette=sns.color_palette(\"hls\", colors),\n",
        "    data=dataset_subset,\n",
        "    hue=\"Label\",\n",
        "    legend=\"full\",\n",
        "    alpha=0.3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "Tg8-Kc4Iu0o8"
      },
      "source": [
        "From the cell above we can see distribution of the data in 2D space. It is obvious that attacks are not spatialy well separated from normal state. Clusters of attacks can hardly be seen, instead they are found in the same place as the \"normal state\" datatpoints.\n",
        "\n",
        "This insight leads us to conclude that the ML model will probably have some issues with this kind of data. ML model will have to be chosen with this in mind."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "4eAG_UPVu0o8"
      },
      "source": [
        "# Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "ybFx65Zqu0o8"
      },
      "source": [
        "In this chapter, final data preparation steps are taken before we use the data for model traning and testing.\n",
        "\n",
        "These steps include:\n",
        "\n",
        "* Data scaling\n",
        "* Label encoding\n",
        "* Data splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "5MZmsa7tu0o8"
      },
      "outputs": [],
      "source": [
        "#dataset = pd.read_csv(\"Dataset_clean_dropna.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "F3yXPyn8u0o9"
      },
      "source": [
        "## Scaling the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "esq36NlWu0o9"
      },
      "source": [
        "The next few cells contain the code for scaling the data into the size adequate for the ML algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "q9Ps7JLGu0o9"
      },
      "outputs": [],
      "source": [
        "# Splitting dataset into features and labels.\n",
        "\n",
        "labels = dataset['Label']\n",
        "features = dataset.loc[:, dataset.columns != 'Label'].astype('float64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "AOQsFQ6au0o9"
      },
      "outputs": [],
      "source": [
        "features.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "e7cVwVAGu0o9"
      },
      "outputs": [],
      "source": [
        "# For scaling the data, we use RobustScaler class from sklearn.\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "6kk-U564u0o-"
      },
      "source": [
        "For scaling the data we used RobustScaler class from sklearn. RobustScaler is used to perserve outliers in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "scrolled": false,
        "id": "pR4YmFj5u0o-"
      },
      "outputs": [],
      "source": [
        "scaler = RobustScaler()\n",
        "scaler.fit(features)\n",
        "\n",
        "features = scaler.transform(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "2fhSQlK4u0o-"
      },
      "outputs": [],
      "source": [
        "# Checking if scaling has been succesful.\n",
        "features[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "mKqMnRPwu0o-"
      },
      "source": [
        "## Label encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "xTF0p3mNu0o-"
      },
      "source": [
        "Label encoding is done when dataset contains categorical values (ex. 0-5, A/B/C, 55+). It is used to turn categorical values into numerical values by replacing data categories with integers starting with 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "scrolled": true,
        "id": "zBjIkKV_u0o-"
      },
      "outputs": [],
      "source": [
        "# No need to do previous operations, just load clean saved dataset.\n",
        "\n",
        "#dataset = pd.read_csv('Dataset_clean.csv', index_col=[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "AMVFzzHJu0o_"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "vbqd5qx-u0o_"
      },
      "source": [
        "'Lables' column contains categorical values - 15 of them (14 types of attacks in our dataset +  1 normal state).\n",
        "\n",
        "To convert this into numerical values we will use 'LabelEncoder' class from sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "YVXnxd9Eu0o_"
      },
      "outputs": [],
      "source": [
        "LE = LabelEncoder()\n",
        "\n",
        "LE.fit(labels)\n",
        "labels = LE.transform(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "YvHfjdEhu0o_"
      },
      "outputs": [],
      "source": [
        "# Labels have been replaced with integers.\n",
        "\n",
        "np.unique(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "ObEpHrA3u0o_"
      },
      "outputs": [],
      "source": [
        "# Checking that encoding reversal works.\n",
        "\n",
        "d = LE.inverse_transform(labels)\n",
        "d = pd.Series(d)\n",
        "d.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "wIKqO1szu0pA"
      },
      "source": [
        "## Splitting the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "-xII2u9Lu0pA"
      },
      "source": [
        "Final step to data preparation is splitting the data into traning and testing sets. For this there already exists _sklearn_ function that does all the splitting for us. This step is important so we can have representative data for evaluating our model. Both train and test samples should contain similar data variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "8sT1Ed2Qu0pA"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "scrolled": false,
        "id": "JiSulZXFu0pA"
      },
      "outputs": [],
      "source": [
        "# The next step is to split training and testing data. For this we will use sklearn function train_test_split().\n",
        "\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=.2)\n",
        "\n",
        "features_train.shape, features_test.shape, labels_train.shape, labels_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "8E3hld5ju0pB"
      },
      "outputs": [],
      "source": [
        "# Clearing variables.\n",
        "\n",
        "dataset = None\n",
        "finite = None\n",
        "labl = None\n",
        "d = None\n",
        "features = None\n",
        "labels = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "QSI9oonEu0pB"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "yTQfmU34u0pB"
      },
      "source": [
        "For completing this task we chose to use a neural network. Specifically, the multi-layer perceptron, more specifically, feedforward neural network multi-class classifier with backpropagating algorithm. NN will be used to classifiy 14 different attacks and 1 normal state, as we saw from the labels in previous chapters.\n",
        "\n",
        "In this chapter we go by explaning parts of the network and its hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "Q1XI6Sn8u0pB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import datetime\n",
        "\n",
        "#%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "t8VtGLcEu0pB"
      },
      "source": [
        "Our tensorflow Sequential model has 3 layers. Input, 1 hidden and an output layer.\n",
        "\n",
        "* Input layer has 78 neurons, one for each feature.\n",
        "* Hidden layer has 67 neurons, this number has been calculated by [formula](https://www.heatonresearch.com/2017/06/01/hidden-layers.html) 2/3 the number of input neurons + number of output neurons.\n",
        "* Output layer has 15 neurons, one for each class we predict.\n",
        "\n",
        "For activation functions, we used standard functions for multi-class classification tasks - ReLu for hidden layer and _softmax_ function for output layer.\n",
        "\n",
        "Finally, we use Dropout parameter set to 0.2 for randomly shutting off 20% of neurons in each learning iteration. This technique is used for decreasing overfitting thereby incresing network accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "wE0hKjJfu0pC"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "\n",
        "    tf.keras.layers.Flatten(input_shape=(78,)),\n",
        "    tf.keras.layers.Dense(67, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(15, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "GutCua-Su0pC"
      },
      "source": [
        "For learning rate optimization we used Adam optimizer.\n",
        "Loss function used is sparse categorical crossentropy, which is standard for multiclass classification problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "GKjingGlu0pC"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "             loss='sparse_categorical_crossentropy',\n",
        "             metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "ARfTIbCmu0pC"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "yWncjJzQu0pC"
      },
      "source": [
        "In the next cell we setup training logs for tensorboard as well as some tensorboard callbacks.\n",
        "\n",
        "* tensorboard - callback that logs training data.\n",
        "* EarlyStopping - callback that monitors 'loss (function)' metric and if the loss function does not get better in tne hext 10 iterations, callback stops the training and resotres the network with best weights up untill that iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "lfuqWpIou0pD"
      },
      "outputs": [],
      "source": [
        "log_dir = os.path.join(\n",
        "    \"train_logs\",\n",
        "    datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
        ")\n",
        "\n",
        "# TF callback that sets up TensorBoard with training logs.\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "# TF callback that stops training when best value of validationi loss function is reached. It also\n",
        "# restores weights from the best training iteration.\n",
        "eary_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "yNtluy5Mu0pD"
      },
      "outputs": [],
      "source": [
        "model.fit(features_train,\n",
        "          labels_train,\n",
        "          epochs=100,\n",
        "          callbacks=[tensorboard_callback, eary_stop_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "mR_KfFJfu0pD"
      },
      "source": [
        "We can see that training stoped after 18 out of 100 epochs due to 'loss' function metric not changing much in the previous 10 epochs.\n",
        "\n",
        "After training we evaluate model accuracy (next cell), and find that our model predicts attacks with **91.2% accuracy**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "1GDaNylau0pD"
      },
      "outputs": [],
      "source": [
        "# Evaluating model accuracy.\n",
        "model.evaluate(features_test, labels_test, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "wZQHlmVSu0pE"
      },
      "outputs": [],
      "source": [
        "# Saving the model.\n",
        "\n",
        "model.save('saved_models/IDS_model_' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "TI334yHUu0pE"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "x5rmBXpDu0pE"
      },
      "source": [
        "In this project we made a neural network classifier that can predict 14 network/web attacks and normal traffic with 91% accuracy. This model is proof-of-concept that feedforward neural network with backpropagation algorithm can be used for classifying attacks in anomaly-based intrusion detection systems.\n",
        "\n",
        "\n",
        "**Propositions**\n",
        "\n",
        "We propose a couple of solutions for improving model accuracy as well as use of some other neural network architectures.\n",
        "\n",
        "Accuracy of this model can probably be improved by _feature engineering_ and _feature selection_. Picking the features that have the most influence on the model.\n",
        "\n",
        "Regarding this model, we propose tuning the model hyperparameters. Changing the hidden layer activation function, early stopping callback, dropout, optimizer and loss function should increase accuracy by some extent. Another way, albeit more complicated and resource intense is to use a genetic algorithm to evolve the best neural network arhitecture for this specific task.\n",
        "\n",
        "Finally, we propose the usage of some other ML algorithms. Random forest classifiers have been used in intrusion detection system for a while now. Alternatively, we found some sources using autoencoders for anomaly detection."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "375.994px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "position": {
        "height": "485.188px",
        "left": "645.125px",
        "right": "20px",
        "top": "174.938px",
        "width": "628.75px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}